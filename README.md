# Quantization-Memory-Effecient-Neural-Networks
In this repo I have created 4 different version of a simple big dimensional multi layer perceptron. These 4 versions all aim to reduce memory during training by playing with the weights and architecture of the network. I have implemented half linear, low rank adapter (LoRA), low precision, and quantized LoRA.
